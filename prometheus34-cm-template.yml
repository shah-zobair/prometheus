apiVersion: v1
data:
  alerting.rules: |
    groups:
    - name: example-rules
      interval: 30s # defaults to global interval
      rules:
      - alert: Node Down
        expr: up{job="kubernetes-nodes"} == 0
        annotations:
          miqTarget: "ContainerNode"
          severity: "HIGH"
          message: "{{$labels.instance}} is down"
      - alert: "Too Many Pods"
        expr: sum(kubelet_running_pod_count) > 4000
        annotations:
          miqTarget: "ExtManagementSystem"
          severity: "ERROR"
          message: "Too many running pods"
      - alert: "Node CPU Usage"
        expr: (100 - (avg by (instance) (irate(node_cpu{job="kubernetes-nodes-exporter",mode="idle"}[5m])) * 100)) > 10
        for: 30s
        labels:
          severity: "ERROR"
        annotations:
          miqTarget: "ExtManagementSystem"
          severity: "ERROR"
          message: "{{$labels.instance}}: CPU usage is above 4% (current value is: {{ $value }})"
  prometheus.yml: |
    rule_files:
      - '*.rules'

    # A scrape configuration for running Prometheus on a Kubernetes cluster.
    # This uses separate scrape configs for cluster components (i.e. API server, node)
    # and services to allow each to use different authentication configs.
    #
    # Kubernetes labels will be added as Prometheus labels on metrics via the
    # `labelmap` relabeling action.

    # Scrape config for API servers.
    #
    # Kubernetes exposes API servers as endpoints to the default/kubernetes
    # service so this uses `endpoints` role and uses relabelling to only keep
    # the endpoints associated with the default/kubernetes service using the
    # default named port `https`. This works for single API server deployments as
    # well as HA API server deployments.
    scrape_configs:
    - job_name: 'kubernetes-apiservers'

      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - default

      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # Keep only the default/kubernetes service endpoints for the https port. This
      # will add targets for each API server which Kubernetes adds an endpoint to
      # the default/kubernetes service.
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: kubernetes;https

    # Scrape config for controllers.
    #
    # Each master node exposes a /metrics endpoint on :8444 that contains operational metrics for
    # the controllers.
    #
    # TODO: move this to a pure endpoints based metrics gatherer when controllers are exposed via
    #       endpoints.
    - job_name: 'kubernetes-controllers'

      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - default

      # Keep only the default/kubernetes service endpoints for the https port, and then
      # set the port to 8444. This is the default configuration for the controllers on OpenShift
      # masters.
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: kubernetes;https
      - source_labels: [__address__]
        action: replace
        target_label: __address__
        regex: (.+)(?::\d+)
        replacement: $1:8444

    # Scrape config for nodes.
    #
    # Each node exposes a /metrics endpoint that contains operational metrics for
    # the Kubelet and other components.
    - job_name: 'kubernetes-nodes'

      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      kubernetes_sd_configs:
      - role: node

      # Drop a very high cardinality metric that is incorrect in 3.7. It will be
      # fixed in 3.9.
      metric_relabel_configs:
      - source_labels: [__name__]
        action: drop
        regex: 'openshift_sdn_pod_(setup|teardown)_latency(.*)'

      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)


    # Scrape config for service endpoints.
    #
    # The relabeling allows the actual service scrape endpoint to be configured
    # via the following annotations:
    #
    # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
    # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
    # to set this to `https` & most likely set the `tls_config` of the scrape config.
    # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
    # * `prometheus.io/port`: If the metrics are exposed on a different port to the
    # service then set this appropriately.
    - job_name: 'kubernetes-service-endpoints'

      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        # TODO: this should be per target
        insecure_skip_verify: true

      kubernetes_sd_configs:
      - role: endpoints

      relabel_configs:
      # only scrape infrastructure components
      - source_labels: [__meta_kubernetes_namespace]
        action: keep
        regex: 'default|logging|metrics|kube-.+|openshift|prometheus|openshift-.+'
      # drop infrastructure components managed by other scrape targets
      - source_labels: [__meta_kubernetes_service_name]
        action: drop
        regex: 'prometheus-node-exporter'
      # only those that have requested scraping
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme__
        regex: (https?)
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: (.+)(?::\d+);(\d+)
        replacement: $1:$2
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name


    # Scrape config for BlackBox
    - job_name: 'blackbox'
      metrics_path: /probe
      params:
        module: [http_2xx]  # Look for a HTTP 200 response.
      static_configs:
        - targets:
          - https://MASTER-URL:MASTER-PORT/healthz
          - https://REGISTRY-URL/healthz
          - http://ROUTER-URL/healthz
      relabel_configs:
        - source_labels: [__address__]
          target_label: __param_target
        - source_labels: [__param_target]
          target_label: instance
        - target_label: __address__
          replacement: blackbox:9115  # The blackbox exporter's real hostname:port.



    # Scrape config for node-exporter, which is expected to be running on port 9100.
    - job_name: 'kubernetes-nodes-exporter'

      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

      kubernetes_sd_configs:
      - role: node

      relabel_configs:
      - source_labels: [device]
        regex: '(/dev/mapper|veth|vxlan).*'
        action: drop
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:9100'
        target_label: __address__
      - source_labels: [__meta_kubernetes_node_label_kubernetes_io_hostname]
        target_label: __instance__
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "localhost:9093"
  recording.rules: |
    groups:
    - name: aggregate_container_resources
      rules:
      - record: container_cpu_usage_rate
        expr: sum without (cpu) (rate(container_cpu_usage_seconds_total[5m]))
      - record: container_memory_rss_by_type
        expr: container_memory_rss{id=~"/|/system.slice|/kubepods.slice"} > 0
      - record: container_cpu_usage_percent_by_host
        expr: sum(rate(container_cpu_usage_seconds_total{id="/"}[5m])) BY(kubernetes_io_hostname) / ON(kubernetes_io_hostname) machine_cpu_cores
      - record: apiserver_request_count_rate_by_resources
        expr: sum without (client,instance,contentType) (rate(apiserver_request_count[5m]))
kind: ConfigMap
metadata:
  name: prometheus
